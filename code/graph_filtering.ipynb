{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import os.path as osp\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset files: ['inference_validation.txt', 'train.txt', 'inference.txt', 'inference_test.txt']\n",
      "num entities of ../data/ilpc/raw/small: 16883\n",
      "num relations of ../data/ilpc/raw/small: 48\n"
     ]
    }
   ],
   "source": [
    "# --- load relevant entities from the LP dataset ---\n",
    "# ilpc small\n",
    "lp_path = '../data/ilpc/raw/small'\n",
    "# ilpc large\n",
    "#lp_path = '../data/ilpc/raw/large'\n",
    "# wikidata5m_inductive\n",
    "#lp_path = '../data/wikidata5m_inductive'\n",
    "# FB15k-237\n",
    "#lp_path = '../data/fb15k-237'\n",
    "\n",
    "\n",
    "lp_files = [x for x in os.listdir(lp_path) if x.endswith('.txt')]\n",
    "print('dataset files:', lp_files)\n",
    "lp_entities = []\n",
    "lp_relations = []\n",
    "for file in lp_files:\n",
    "    triple_table = pd.read_csv(osp.join(lp_path, file), delimiter='\\t', header=None)\n",
    "    lp_entities.extend(triple_table[0].tolist() + triple_table[2].tolist())\n",
    "    lp_relations.extend(triple_table[1].tolist())\n",
    "\n",
    "entity_mentions = collections.Counter(lp_entities)\n",
    "entity_mention_counts = sorted(entity_mentions.values(), reverse=True)\n",
    "\n",
    "lp_entities = set(lp_entities)\n",
    "lp_relations = set(lp_relations)\n",
    "print(f'num entities of {lp_path}:', len(lp_entities))\n",
    "print(f'num relations of {lp_path}:', len(lp_relations))\n",
    "\n",
    "# lp_entities, lp_relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "graph_type = 'triples'  # triples or page_links or all\n",
    "\n",
    "trex_triple_table = pd.read_csv(f'../data/corpus_graphs/trex_{graph_type}.txt', delimiter='\\t', header=None)\n",
    "trex_entities = trex_triple_table[0].tolist() + trex_triple_table[2].tolist()\n",
    "trex_relations = set(trex_triple_table[1].tolist())\n",
    "k = 1  # k-hop\n",
    "degree_filter = 0\n",
    "ratio_to_maintain = 0.5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 5410928/5410928 [00:04<00:00, 1190746.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5703380820903514\n",
      "16883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5410928/5410928 [00:02<00:00, 2576187.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54844\n",
      "Entities maintained: 0.019448461435350638\n",
      "53326\n"
     ]
    }
   ],
   "source": [
    "# todo adjust node degree to the relevant relations\n",
    "# compute degree for each entity in our trex graph\n",
    "relevant_relations = set(list(lp_relations)) #  + ['PageLink']\n",
    "print(relevant_relations)\n",
    "\n",
    "trex_triples = list(trex_triple_table.itertuples(index=False, name=None))\n",
    "print(trex_triples)\n",
    "\n",
    "entity_mentions = {e: 0 for e in set(trex_entities)}\n",
    "for triple in tqdm(trex_triples):\n",
    "    if triple[1] in relevant_relations or all_relations:\n",
    "        entity_mentions[triple[0]] += 1\n",
    "        entity_mentions[triple[2]] += 1\n",
    "\n",
    "print(np.array(list(entity_mentions.values())).mean())\n",
    "\n",
    "\n",
    "covered_entities = lp_entities.copy()\n",
    "print(len(covered_entities))\n",
    "\n",
    "k_counter = k\n",
    "while k_counter:\n",
    "    covered_entities_new = []\n",
    "    for triple in tqdm(trex_triples):\n",
    "        if triple[0] in covered_entities:\n",
    "            if entity_mentions[triple[0]] < degree_filter:\n",
    "                if triple[1] in relevant_relations:\n",
    "                    if random.uniform(0, 1.0) > ratio_to_maintain:\n",
    "                        covered_entities_new.extend([str(triple[0]), str(triple[2])])\n",
    "        if triple[2] in covered_entities:\n",
    "            if entity_mentions[triple[2]] < degree_filter:\n",
    "                if triple[1] in relevant_relations:\n",
    "                    if random.uniform(0, 1.0) > ratio_to_maintain:\n",
    "                        covered_entities_new.extend([str(triple[0]), str(triple[2])])\n",
    "\n",
    "    covered_entities = set(covered_entities).union(covered_entities_new)\n",
    "    print(len(covered_entities))\n",
    "    k_counter -= 1\n",
    "\n",
    "print('Entities maintained:', len(covered_entities)/len(set(trex_entities)))\n",
    "print(len(set(covered_entities).intersection(set(trex_entities))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "triples_filtered = []\n",
    "test_graph = '../data/ilpc/raw/small/inference_test.txt'\n",
    "test_triple_df = pd.read_csv(test_graph, delimiter='\\t', header=None)\n",
    "test_triples = list(test_triple_df.itertuples(index=False, name=None))\n",
    "\n",
    "print(len(test_triples))\n",
    "\n",
    "for triple in tqdm(trex_triples):\n",
    "        if triple[0] in covered_entities and triple[2] in covered_entities and (all_relations or triple[1] in relevant_relations):\n",
    "            if triple not in test_triples:\n",
    "                triples_filtered.append(triple)\n",
    "\n",
    "print('number of triples:', len(triples_filtered))\n",
    "print('Triples maintained:', len(triples_filtered)/len(trex_triples))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
